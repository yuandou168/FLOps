{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Federated Averaging (FedAvg)\n",
    "\n",
    "This is the most common aggregation strategy used in federated learning, and it's implemented in the original paper by Google. FedAvg takes the average of the model weights (or gradients) from all clients.\n",
    "\n",
    "#### Formula:\n",
    "For each model parameter $\\omega$, the new global model parameter is updated by the weighted average of the client models' parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg(models):\n",
    "    \"\"\"\n",
    "    Aggregates models using Federated Averaging (FedAvg).\n",
    "    Args:\n",
    "        models: List of model state_dicts from clients.\n",
    "    Returns:\n",
    "        avg_model: The averaged global model.\n",
    "    \"\"\"\n",
    "    avg_model = models[0].copy()\n",
    "    for key in avg_model.keys():\n",
    "        avg_model[key] = sum([model[key] for model in models]) / len(models)\n",
    "    return avg_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weighted Federated Averaging (Weighted FedAvg)\n",
    "\n",
    "In this variant of FedAvg, the aggregation considers the size of each client's dataset. Clients with more data contribute more to the global model update.\n",
    "\n",
    "#### Formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_fed_avg(models, client_sizes):\n",
    "    \"\"\"\n",
    "    Aggregates models using Weighted Federated Averaging.\n",
    "    Args:\n",
    "        models: List of model state_dicts from clients.\n",
    "        client_sizes: List of dataset sizes for each client.\n",
    "    Returns:\n",
    "        avg_model: The averaged global model.\n",
    "    \"\"\"\n",
    "    total_size = sum(client_sizes)\n",
    "    avg_model = models[0].copy()\n",
    "\n",
    "    for key in avg_model.keys():\n",
    "        avg_model[key] = sum([model[key] * (client_sizes[i] / total_size) for i, model in enumerate(models)])\n",
    "    return avg_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Median Aggregation (FedMedian)\n",
    "Instead of averaging model weights, FedMedian uses the median of the client models' parameters. This strategy is more robust to outliers and may help in cases where some clients have noisy or malicious data.\n",
    "\n",
    "#### Formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fed_median(models):\n",
    "    \"\"\"\n",
    "    Aggregates models using Federated Median (FedMedian).\n",
    "    Args:\n",
    "        models: List of model state_dicts from clients.\n",
    "    Returns:\n",
    "        median_model: The median global model.\n",
    "    \"\"\"\n",
    "    median_model = models[0].copy()\n",
    "\n",
    "    for key in median_model.keys():\n",
    "        # Stack all model parameters along a new axis and compute the median along that axis\n",
    "        stacked_weights = np.stack([model[key].cpu().numpy() for model in models])\n",
    "        median_model[key] = torch.tensor(np.median(stacked_weights, axis=0))\n",
    "\n",
    "    return median_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trimmed Mean Aggregation (Trimmed Mean)\n",
    "The Trimmed Mean strategy trims the highest and lowest values for each parameter and then takes the mean of the remaining values. This is another robust approach that mitigates the effect of outliers or malicious clients.\n",
    "\n",
    "#### Formula:\n",
    "For each parameter, remove the highest and lowest $ùëû$% of the values and then average the remaining values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmed_mean(models, trim_percent=0.1):\n",
    "    \"\"\"\n",
    "    Aggregates models using Trimmed Mean.\n",
    "    Args:\n",
    "        models: List of model state_dicts from clients.\n",
    "        trim_percent: Percentage of extreme values to trim from each side (default 10%).\n",
    "    Returns:\n",
    "        trimmed_mean_model: The trimmed mean global model.\n",
    "    \"\"\"\n",
    "    trimmed_mean_model = models[0].copy()\n",
    "    trim_count = int(trim_percent * len(models))\n",
    "\n",
    "    for key in trimmed_mean_model.keys():\n",
    "        # Stack all model parameters along a new axis\n",
    "        stacked_weights = np.stack([model[key].cpu().numpy() for model in models])\n",
    "\n",
    "        # Sort the weights and remove the top and bottom trim_percent of the values\n",
    "        sorted_weights = np.sort(stacked_weights, axis=0)\n",
    "        trimmed_weights = sorted_weights[trim_count:-trim_count]\n",
    "\n",
    "        # Compute the mean of the trimmed weights\n",
    "        trimmed_mean_model[key] = torch.tensor(np.mean(trimmed_weights, axis=0))\n",
    "\n",
    "    return trimmed_mean_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Norm-based Clipping Aggregation\n",
    "This aggregation strategy clips the model weights based on their norms. This is useful when some clients have unusually large weight updates, which can destabilize training.\n",
    "\n",
    "#### Formula:\n",
    "For each model parameter $\\omega_k$, clip it if its norm exceeds a threshold ùúè:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_clipping(models, clip_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Aggregates models using Norm-based Clipping.\n",
    "    Args:\n",
    "        models: List of model state_dicts from clients.\n",
    "        clip_threshold: Clipping threshold for the norm.\n",
    "    Returns:\n",
    "        clipped_model: The clipped global model.\n",
    "    \"\"\"\n",
    "    clipped_model = models[0].copy()\n",
    "\n",
    "    for key in clipped_model.keys():\n",
    "        stacked_weights = np.stack([model[key].cpu().numpy() for model in models])\n",
    "        norm_weights = np.linalg.norm(stacked_weights, axis=0)\n",
    "\n",
    "        # Clip the weights based on their norm\n",
    "        clipped_weights = np.minimum(1, clip_threshold / norm_weights) * stacked_weights\n",
    "        clipped_model[key] = torch.tensor(np.mean(clipped_weights, axis=0))\n",
    "\n",
    "    return clipped_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Krum Aggregation\n",
    "Krum is an aggregation strategy designed to be robust to Byzantine (malicious) clients. It selects a single model from the set of client models that is the \"most central\" by calculating distances between model updates and excluding outliers.\n",
    "\n",
    "#### Formula:\n",
    "For each model, compute the distance to all other models and select the one with the smallest sum of distances to its closest $K$ neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def krum(models, num_neighbors=2):\n",
    "    \"\"\"\n",
    "    Aggregates models using Krum, a robust aggregation technique.\n",
    "    Args:\n",
    "        models: List of model state_dicts from clients.\n",
    "        num_neighbors: Number of closest neighbors to consider (default is 2).\n",
    "    Returns:\n",
    "        krum_model: The selected Krum model.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "\n",
    "    # Calculate distances between models\n",
    "    for i, model_i in enumerate(models):\n",
    "        dists = []\n",
    "        for j, model_j in enumerate(models):\n",
    "            if i != j:\n",
    "                dist = sum([(model_i[key] - model_j[key]).norm().item() for key in model_i])\n",
    "                dists.append(dist)\n",
    "        dists.sort()\n",
    "        distances.append((i, sum(dists[:num_neighbors])))\n",
    "\n",
    "    # Select the model with the smallest sum of distances to its closest neighbors\n",
    "    selected_model_index = sorted(distances, key=lambda x: x[1])[0][0]\n",
    "    return models[selected_model_index]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
